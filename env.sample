ANTHROPIC_API_KEY=your_anthropic_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
PRIMARY_LLM=anthropic            # anthropic or openai

# Anthropic model config
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_MAX_TOKENS=2000        # max tokens for Anthropic response
ANTHROPIC_TEMPERATURE=0.1

# OpenAI model config
OPENAI_MODEL=gpt-4o
OPENAI_MAX_TOKENS=2000           # max tokens for OpenAI responses
OPENAI_TEMPERATURE=0.1

DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres
REDIS_URL=redis://localhost:6379/0
CHROMADB_URL=http://localhost:8001
CHROMADB_COLLECTION=security_knowledge_base

# Application Config
MAX_CONCURRENT_ANALYSES=5
CACHE_TTL_SECONDS=2592000        # cache TTL in seconds (2592000 = 30 days)
MAX_RETRIES=3                    # max number of retries for LLM calls

# RAG Config
EMBEDDING_MODEL=all-MiniLM-L6-v2 # embedding model for vector search
CHUNK_SIZE=1000                  # chunk size for knowledge base
CHUNK_OVERLAP=200                # chunk overlap
RETRIEVAL_K=5                    # no of chunks to retrieve for context

# Prompt Config
# Default analysis type (general, trust_policy, resource_specific, compliance, risk_assessment)
DEFAULT_ANALYSIS_TYPE=general
INCLUDE_BUSINESS_CONTEXT=true # Include business context in analysis
INCLUDE_COMPLIANCE_MAPPING=true # Include compliance framework mapping
INCLUDE_RISK_SCORING=true # Include risk scoring in analysis
INCLUDE_REMEDIATION_COMMANDS=true # Include remediation CLI commands
INCLUDE_CONFIDENCE_SCORING=true # Include confidence scoring
MAX_CONTEXT_LENGTH=4000 # Maximum context length for prompts

# Analysis Config
# Analysis focus areas (comma-separated)
ANALYSIS_FOCUS_AREAS=least_privilege_violations,privilege_escalation_risks,compliance_violations,resource_wildcards

# Compliance frameworks to check (comma-separated)
COMPLIANCE_FRAMEWORKS=CIS,NIST,SOC2

# Risk assessment strictness (low, medium, high)
RISK_ASSESSMENT_STRICTNESS=medium

# API version string
API_V1_STR=/api/v1

# Project name
PROJECT_NAME=IAM Scanner

# =============================================================================
# AWS CONFIGURATION (Optional - can be set via API)
# =============================================================================
# AWS Config
DEFAULT_AWS_ACCOUNT_ID=123456789012
DEFAULT_AWS_ROLE_ARN=arn:aws:iam::123456789012:role/StackGuardScannerRole
AWS_EXTERNAL_ID=stackguard-scanner-a1b2c3d4e5

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================
# Secret key for JWT tokens (generate a secure random string)
SECRET_KEY=your-secret-key-here

# Algorithm for JWT tokens
ALGORITHM=HS256

# Access token expiration time in minutes
ACCESS_TOKEN_EXPIRE_MINUTES=30


# =============================================================================
# MONITORING CONFIGURATION
# =============================================================================
# Enable health checks
ENABLE_HEALTH_CHECKS=true

# Health check interval in seconds
HEALTH_CHECK_INTERVAL=30

# =============================================================================
# CELERY CONFIGURATION
# =============================================================================
# Celery broker URL (same as Redis URL)
CELERY_BROKER_URL=redis://localhost:6379/0

# Celery result backend URL (same as Redis URL)
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# Celery task serialization
CELERY_TASK_SERIALIZER=json

# Celery result serialization
CELERY_RESULT_SERIALIZER=json

# Celery accept content
CELERY_ACCEPT_CONTENT=json

# Celery timezone
CELERY_TIMEZONE=UTC

# Celery enable UTC
CELERY_ENABLE_UTC=true

# =============================================================================
# DOCKER CONFIGURATION
# =============================================================================
# Docker compose project name
COMPOSE_PROJECT_NAME=iam-scanner

# Docker network name
DOCKER_NETWORK=iam-scanner-network

# =============================================================================
# EXAMPLE VALUES FOR DIFFERENT ENVIRONMENTS
# =============================================================================

# Development Environment Example:
# DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres
# REDIS_URL=redis://localhost:6379/0
# CHROMADB_URL=http://localhost:8001
# ANTHROPIC_API_KEY=sk-ant-api03-...
# OPENAI_API_KEY=sk-...

# Docker Environment Example:
# DATABASE_URL=postgresql://postgres:postgres@postgres:5432/postgres
# REDIS_URL=redis://redis:6379/0
# CHROMADB_URL=http://chromadb:8000
# ANTHROPIC_API_KEY=sk-ant-api03-...
# OPENAI_API_KEY=sk-...

# Production Environment Example:
# DATABASE_URL=postgresql://user:password@prod-db-host:5432/iam_scanner
# REDIS_URL=redis://user:password@prod-redis-host:6379/0
# CHROMADB_URL=http://prod-chromadb-host:8000
# ANTHROPIC_API_KEY=sk-ant-api03-...
# OPENAI_API_KEY=sk-...
# DEBUG=false
# RELOAD=false

# =============================================================================
# EXAMPLE MODEL CONFIGURATIONS
# =============================================================================

# High Performance Configuration (Most Capable Models):
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
# OPENAI_MODEL=gpt-4o
# ANTHROPIC_MAX_TOKENS=4000
# OPENAI_MAX_TOKENS=4000
# ANTHROPIC_TEMPERATURE=0.1
# OPENAI_TEMPERATURE=0.1

# Cost-Optimized Configuration (Faster, Cheaper Models):
# ANTHROPIC_MODEL=claude-3-5-haiku-20241022
# OPENAI_MODEL=gpt-4o-mini
# ANTHROPIC_MAX_TOKENS=1000
# OPENAI_MAX_TOKENS=1000
# ANTHROPIC_TEMPERATURE=0.2
# OPENAI_TEMPERATURE=0.2

# Development Configuration (Fastest Models):
# ANTHROPIC_MODEL=claude-3-5-haiku-20241022
# OPENAI_MODEL=gpt-3.5-turbo
# ANTHROPIC_MAX_TOKENS=500
# OPENAI_MAX_TOKENS=500
# ANTHROPIC_TEMPERATURE=0.3
# OPENAI_TEMPERATURE=0.3

# Available Anthropic models:
# - claude-3-5-sonnet-20241022 (Latest, most capable)
# - claude-3-5-haiku-20241022 (Faster, cheaper)
# - claude-3-opus-20240229 (Previous generation)

# Available OpenAI models:
# - gpt-4o (Latest, most capable)
# - gpt-4o-mini (Faster, cheaper)
# - gpt-4-turbo (Previous generation)
# - gpt-3.5-turbo (Fastest, cheapest)
